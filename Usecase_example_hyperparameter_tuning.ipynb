{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from custom_ml_toolkit.preprocessor.encoder import SupportMissingDatasetEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import optuna\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 77\n",
    "data_df = pd.read_csv('example_data/titanic.csv')\n",
    "data_df['Deck'] = data_df['Cabin'].str[0]\n",
    "\n",
    "numerical_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "norminal_cols = ['Sex', 'Embarked']\n",
    "ordinal_cols = ['Pclass', 'Deck']\n",
    "target_col = 'Survived'\n",
    "\n",
    "train_data_df, test_data_df = train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.5,\n",
    "    random_state=random_state,\n",
    "    stratify=data_df['Survived']\n",
    ")\n",
    "\n",
    "de = SupportMissingDatasetEncoder(\n",
    "    numerical_cols=numerical_cols,\n",
    "    norminal_cols=norminal_cols,\n",
    "    ordinal_cols=ordinal_cols,\n",
    "    target_col=target_col,\n",
    "    drop_binary=True,\n",
    "    oe_unknown_value=np.nan,\n",
    "    oe_missing_value=np.nan,\n",
    "    encode_target=True\n",
    ")\n",
    "\n",
    "de.fit(train_data_df)\n",
    "encoded_train_data_df = de.transform(train_data_df)\n",
    "encoded_test_data_df = de.transform(test_data_df)\n",
    "\n",
    "X_train = encoded_train_data_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_data_df['Survived']\n",
    "\n",
    "X_test = encoded_test_data_df.drop(columns=['Survived'])\n",
    "y_test = encoded_test_data_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_weight(\n",
    "        trial,\n",
    "        class_names: list\n",
    "    ):\n",
    "    class_names = sorted(class_names)\n",
    "    class_weights_dict = dict()\n",
    "\n",
    "    if len(class_names) > 2:\n",
    "        for class_name in class_names:\n",
    "            class_weights_dict[class_name] = trial.suggest_float(class_name, 0.01, 1)\n",
    "    else:\n",
    "        first_class_weight = trial.suggest_float(class_names[0], 0.01, 1)\n",
    "        second_calss_weight = 1 - first_class_weight\n",
    "        class_weights_dict[class_names[0]] = first_class_weight\n",
    "        class_weights_dict[class_names[1]] = second_calss_weight\n",
    "\n",
    "    return class_weights_dict\n",
    "\n",
    "def generate_hyper_params(\n",
    "        trial,\n",
    "        model: Literal['xgb', 'lgbm'] = 'xgb'\n",
    "    ):\n",
    "    if model == 'xgb':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 900),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 1, log=True), # or eta def=0.3 [0,1]\n",
    "            'min_split_loss': trial.suggest_float('min_split_loss', 0, 10), # or gamma def=0 [0, inf]\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 10), # def=6 [0, inf]\n",
    "            'max_leaves': trial.suggest_int('max_leaves', 0, 10), # def=0\n",
    "            'max_bin': trial.suggest_int('max_bin', 128, 256), # def=256\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), # def=1 [0, inf]\n",
    "            'max_delta_step': trial.suggest_int('max_delta_step', 1, 10), # def=1 [0, inf]\n",
    "            'subsample': trial.suggest_float('subsample', 0.01, 1.0, log=True), # def=1 (0, inf]\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1, log=True), # def=1 (0, 1]\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 2.0, log=True), # or lambda def=1\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 2.0, log=True), # or alpha def=0\n",
    "            ## 'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "            ## 'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear']), # def=gbtree\n",
    "            ## 'sampling_method' trial.suggest_categorical('sampling_method', ['uniform', 'gradient_based'] ) # def=uniform only supported when tree_method is set to gpu_hist\n",
    "            ##'tree_method': trial.suggest_categorical('tree_method', ['exact', 'approx', 'hist']), # def=auto\n",
    "        }\n",
    "    elif model == 'lgbm':\n",
    "        params = {\n",
    "        }\n",
    "    return params\n",
    "\n",
    "def train_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        params: dict = None,\n",
    "        class_weights: dict = None,\n",
    "        model: Literal['xgb', 'lgbm'] = 'xgb'\n",
    "    ):\n",
    "\n",
    "    if params is None:\n",
    "        params = dict()\n",
    "\n",
    "    if class_weights is None:\n",
    "        sample_weight = None\n",
    "    else:\n",
    "        sample_weight = y_train\\\n",
    "            .map(class_weights)\\\n",
    "            .to_numpy()\n",
    "\n",
    "    if model == 'xgb':\n",
    "        clf = XGBClassifier(\n",
    "            **params,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            missing=np.nan\n",
    "        )\n",
    "    elif model == 'lgbm':\n",
    "        clf = LGBMClassifier(\n",
    "            **params,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        sample_weight=sample_weight\n",
    "    )\n",
    "\n",
    "    return clf\n",
    "\n",
    "def eval_model(clf, X_test, y_test):\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    eval_dict = classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_test_pred,\n",
    "        output_dict=True\n",
    "    )\n",
    "    score = eval_dict['macro avg']['f1-score']\n",
    "    return score\n",
    "\n",
    "def objective(\n",
    "        trial,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test=None,\n",
    "        y_test=None,\n",
    "        k=3,\n",
    "        model: Literal['xgb', 'lgbm'] = 'xgb',\n",
    "        mode: Literal['class_weight', 'params'] = 'params',\n",
    "        class_weights:dict = None\n",
    "    ):\n",
    "\n",
    "    if mode == 'class_weight':\n",
    "        class_weights = generate_class_weight(trial, class_names=list(y_train.unique()))\n",
    "        params = None\n",
    "    elif mode == 'params':\n",
    "        params = generate_hyper_params(trial, model=model)\n",
    "\n",
    "    if (X_test is None) or (y_test is None):\n",
    "        kf = KFold(n_splits=k, shuffle=False)\n",
    "        cv_score_list = list()\n",
    "        for train_index, test_index in kf.split(X=X_train, y=y_train):\n",
    "            X_train_cv = X_train.iloc[train_index].copy()\n",
    "            y_train_cv =  y_train.iloc[train_index].copy()\n",
    "            X_test_cv = X_train.iloc[test_index].copy()\n",
    "            y_test_cv = y_train.iloc[test_index].copy()\n",
    "\n",
    "            clf = train_model(\n",
    "                    X_train_cv,\n",
    "                    y_train_cv,\n",
    "                    params=params,\n",
    "                    class_weights=class_weights,\n",
    "                    model=model\n",
    "            )\n",
    "            cv_score = eval_model(clf, X_test_cv, y_test_cv)\n",
    "\n",
    "            cv_score_list.append(cv_score)\n",
    "        score = sum(cv_score_list)/len(cv_score_list)\n",
    "    else:\n",
    "        clf = train_model(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                params=params,\n",
    "                class_weights=class_weights,\n",
    "                model=model\n",
    "        )\n",
    "        score = eval_model(clf, X_test, y_test)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 17:58:06,753] A new study created in memory with name: no-name-43622ffa-6fd2-418d-a6dc-6ae5314cc4a9\n",
      "[I 2024-09-17 17:58:07,235] Trial 0 finished with value: 0.38090224302046627 and parameters: {'n_estimators': 821, 'learning_rate': 0.16860186056007417, 'min_split_loss': 2.1831193217922085, 'max_depth': 5, 'max_leaves': 6, 'max_bin': 250, 'min_child_weight': 5, 'max_delta_step': 9, 'subsample': 0.0744661545568553, 'colsample_bytree': 0.04948407690957704, 'reg_lambda': 0.000576661769207997, 'reg_alpha': 0.011414858694728367}. Best is trial 0 with value: 0.38090224302046627.\n",
      "[I 2024-09-17 17:58:07,369] Trial 1 finished with value: 0.38090224302046627 and parameters: {'n_estimators': 188, 'learning_rate': 0.022106427418954033, 'min_split_loss': 4.602211056190209, 'max_depth': 4, 'max_leaves': 0, 'max_bin': 163, 'min_child_weight': 10, 'max_delta_step': 5, 'subsample': 0.02843184554930263, 'colsample_bytree': 0.12076614015091139, 'reg_lambda': 0.002177394281737815, 'reg_alpha': 3.6990911608616824e-05}. Best is trial 0 with value: 0.38090224302046627.\n",
      "[I 2024-09-17 17:58:07,804] Trial 2 finished with value: 0.38090224302046627 and parameters: {'n_estimators': 812, 'learning_rate': 0.011390084448850682, 'min_split_loss': 0.36274389514093786, 'max_depth': 4, 'max_leaves': 1, 'max_bin': 183, 'min_child_weight': 10, 'max_delta_step': 5, 'subsample': 0.034860385814896366, 'colsample_bytree': 0.04689895471853157, 'reg_lambda': 3.930612676017706e-05, 'reg_alpha': 0.00016790241002742298}. Best is trial 0 with value: 0.38090224302046627.\n",
      "[I 2024-09-17 17:58:08,257] Trial 3 finished with value: 0.38090224302046627 and parameters: {'n_estimators': 786, 'learning_rate': 0.04083298923364147, 'min_split_loss': 8.61854729282419, 'max_depth': 3, 'max_leaves': 7, 'max_bin': 223, 'min_child_weight': 6, 'max_delta_step': 7, 'subsample': 0.013749798109085073, 'colsample_bytree': 0.017366096976545065, 'reg_lambda': 0.3903376538910376, 'reg_alpha': 0.08439661011177235}. Best is trial 0 with value: 0.38090224302046627.\n",
      "[I 2024-09-17 17:58:08,703] Trial 4 finished with value: 0.38090224302046627 and parameters: {'n_estimators': 872, 'learning_rate': 0.4566313508469938, 'min_split_loss': 6.5670395919507385, 'max_depth': 4, 'max_leaves': 5, 'max_bin': 164, 'min_child_weight': 10, 'max_delta_step': 2, 'subsample': 0.046466008086983715, 'colsample_bytree': 0.07134083658792185, 'reg_lambda': 0.034568332806260385, 'reg_alpha': 4.084776744045878e-07}. Best is trial 0 with value: 0.38090224302046627.\n",
      "[I 2024-09-17 17:58:09,085] Trial 5 finished with value: 0.7842096937170168 and parameters: {'n_estimators': 691, 'learning_rate': 0.9036246446851729, 'min_split_loss': 4.535285602513708, 'max_depth': 4, 'max_leaves': 5, 'max_bin': 169, 'min_child_weight': 1, 'max_delta_step': 2, 'subsample': 0.7268137928737366, 'colsample_bytree': 0.039145186484405986, 'reg_lambda': 1.0445130589977888e-06, 'reg_alpha': 0.00026282453531538966}. Best is trial 5 with value: 0.7842096937170168.\n",
      "[I 2024-09-17 17:58:09,170] Trial 6 finished with value: 0.7665633103435598 and parameters: {'n_estimators': 75, 'learning_rate': 0.03275707989455629, 'min_split_loss': 4.819899554951034, 'max_depth': 1, 'max_leaves': 5, 'max_bin': 169, 'min_child_weight': 7, 'max_delta_step': 4, 'subsample': 0.5767623376380684, 'colsample_bytree': 0.8089036315126374, 'reg_lambda': 0.010256754581651397, 'reg_alpha': 8.649479316082625e-07}. Best is trial 5 with value: 0.7842096937170168.\n",
      "[I 2024-09-17 17:58:09,360] Trial 7 finished with value: 0.38090224302046627 and parameters: {'n_estimators': 243, 'learning_rate': 0.16924046931516204, 'min_split_loss': 2.2904014730541156, 'max_depth': 2, 'max_leaves': 3, 'max_bin': 130, 'min_child_weight': 6, 'max_delta_step': 7, 'subsample': 0.07581055687597843, 'colsample_bytree': 0.9185701199039191, 'reg_lambda': 3.3632841878769144e-05, 'reg_alpha': 0.016242650784287167}. Best is trial 5 with value: 0.7842096937170168.\n",
      "[I 2024-09-17 17:58:09,460] Trial 8 finished with value: 0.5292536910767623 and parameters: {'n_estimators': 88, 'learning_rate': 0.023043779407442927, 'min_split_loss': 9.708600153312295, 'max_depth': 9, 'max_leaves': 0, 'max_bin': 193, 'min_child_weight': 2, 'max_delta_step': 8, 'subsample': 0.3922535107719471, 'colsample_bytree': 0.08488321655409423, 'reg_lambda': 0.05224721202536885, 'reg_alpha': 4.580590041339191e-05}. Best is trial 5 with value: 0.7842096937170168.\n",
      "[I 2024-09-17 17:58:09,778] Trial 9 finished with value: 0.7760067422067252 and parameters: {'n_estimators': 494, 'learning_rate': 0.04173652347522983, 'min_split_loss': 0.0515400173558378, 'max_depth': 7, 'max_leaves': 9, 'max_bin': 170, 'min_child_weight': 9, 'max_delta_step': 2, 'subsample': 0.5750898084004638, 'colsample_bytree': 0.013395135541589316, 'reg_lambda': 0.0709944775683228, 'reg_alpha': 0.0001670357617641596}. Best is trial 5 with value: 0.7842096937170168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.7842096937170168\n",
      "Best Params: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 691,\n",
       " 'learning_rate': 0.9036246446851729,\n",
       " 'min_split_loss': 4.535285602513708,\n",
       " 'max_depth': 4,\n",
       " 'max_leaves': 5,\n",
       " 'max_bin': 169,\n",
       " 'min_child_weight': 1,\n",
       " 'max_delta_step': 2,\n",
       " 'subsample': 0.7268137928737366,\n",
       " 'colsample_bytree': 0.039145186484405986,\n",
       " 'reg_lambda': 1.0445130589977888e-06,\n",
       " 'reg_alpha': 0.00026282453531538966}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_func = partial(\n",
    "    objective,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    # X_test=X_test,\n",
    "    # y_test=y_test,\n",
    "    k=3,\n",
    "    model='xgb',\n",
    "    mode='params'\n",
    ")\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_func, n_trials = 10)\n",
    "trial = study.best_trial\n",
    "print('Best Score: ', trial.value)\n",
    "print('Best Params: ')\n",
    "trial.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
